{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a11134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import permutations, product\n",
    "from random import seed, shuffle, random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110ef5e-00ea-4607-a944-617a90efb3e4",
   "metadata": {},
   "source": [
    "## NUEVOS CONJUNTOS DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa339473-e63c-47aa-b7d4-f77b74ea8a7f",
   "metadata": {},
   "source": [
    "Para crear los nuevos conjuntos de datos, uno para cada fase del modelo (train, validation y test), se seguirá la siguiente estrategia:\n",
    "\n",
    "- Permutaciones sin repetición. En particular: 4 datasets para train, 3 para validation, 3 para test.\n",
    "- Asignar un tipo a cada dataset (espontáneo, False; ataque, True)\n",
    "- Duplicación de permutaciones invirtiendo el tipo\n",
    "- Barajado de permutaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42b54a-2595-4b03-b43e-f6f997be14e7",
   "metadata": {},
   "source": [
    "En cuanto al tamaño de los conjuntos de datos, se tiene, por un lado:\n",
    "\n",
    "- 4 datasets train -> 24 (4!) x 2 (True/False) = 48 permutaciones\n",
    "\n",
    "- 3 datasets validation -> 6 (3!) x 2 = 12 permutaciones\n",
    "\n",
    "- 3 datasets test -> 3! = 6 (3!) x 2 = 12 permutaciones\n",
    "\n",
    "Por otro lado, cada dataset se transforma en una serie temporal con 30 intervalos. Por lo tanto:\n",
    "\n",
    "- Train: 48 permutaciones x 4 datasets/permutación x 30 intervalos/dataset = 5760 intervalos\n",
    "\n",
    "- Validation: 12 permutaciones x 3 datasets/permutación x 30 intervalos/dataset = 1080 puntos\n",
    "\n",
    "- Test: 12 permutaciones x 3 datasets/permutación x 30 intervalos/dataset = 1080 puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59bc82-a07e-4345-bcf0-26d0bc5a4fc4",
   "metadata": {},
   "source": [
    "### PERMUTACIONES: orden datasets y tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42827e34-a523-442c-81ae-13745be65115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "################### TRAIN ###################\n",
    "#############################################\n",
    "\n",
    "# Establecer la semilla\n",
    "seed(10)\n",
    "\n",
    "# Paso 1: Generar permutaciones de los datasets\n",
    "nums_train = [0, 3, 5, 7]\n",
    "perm_train = list(permutations(nums_train))\n",
    "shuffle(perm_train)\n",
    "\n",
    "# Paso 2: Generar permutaciones del tipo de dataset\n",
    "\n",
    "# Generar una lista de booleanos con 50% True y 50% False (MISMO TAMAÑO QUE perm_train)\n",
    "total_booleans = len(perm_train) * 4  \n",
    "num_true = total_booleans // 2         \n",
    "num_false = total_booleans - num_true  \n",
    "# Crear lista\n",
    "bool_flat_list = [True] * num_true + [False] * num_false\n",
    "# Mezclar aleatoriamente\n",
    "shuffle(bool_flat_list)\n",
    "# Dividir en tuplas de 4 elementos\n",
    "tipoDataset_train = [tuple(bool_flat_list[i:i+4]) for i in range(0, total_booleans, 4)]\n",
    "\n",
    "# Paso 3: Duplicación de permutaciones invirtiendo el tipo (Si antes valía False, ahora vale True y viceversa)\n",
    "\n",
    "# Lista complementaria de tipoDataset_train\n",
    "tipoDataset_train_inv = [tuple(not x for x in comb) for comb in tipoDataset_train]\n",
    "perm_train_extended = perm_train * 2                                    # Duplicamos las permutaciones\n",
    "tipoDataset_train_extended = tipoDataset_train + tipoDataset_train_inv  # Unimos las listas complementarias\n",
    "\n",
    "# Paso 4: Emparejar para no perder la correspondencia, barajamos y extraemos las listas\n",
    "dataset_train = list(zip(perm_train_extended, tipoDataset_train_extended))\n",
    "shuffle(dataset_train)\n",
    "perm_train_extended, tipoDataset_train_extended = zip(*dataset_train)\n",
    "perm_train_extended = list(perm_train_extended)\n",
    "tipoDataset_train_extended = list(tipoDataset_train_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264f205d-1693-46ef-b14d-d9df470bfdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 0, 7, 5), (True, False, True, False)),\n",
       " ((3, 5, 0, 7), (True, True, True, False)),\n",
       " ((3, 7, 0, 5), (True, False, False, True)),\n",
       " ((5, 7, 3, 0), (True, True, False, True)),\n",
       " ((5, 3, 7, 0), (True, False, True, False)),\n",
       " ((7, 5, 3, 0), (False, True, False, False)),\n",
       " ((5, 0, 3, 7), (False, False, False, True)),\n",
       " ((0, 7, 5, 3), (False, False, True, True)),\n",
       " ((0, 5, 3, 7), (True, False, False, False)),\n",
       " ((3, 5, 7, 0), (False, False, False, False)),\n",
       " ((3, 7, 5, 0), (True, True, False, False)),\n",
       " ((7, 0, 3, 5), (False, False, True, False)),\n",
       " ((5, 3, 0, 7), (False, False, False, True)),\n",
       " ((5, 0, 7, 3), (True, False, True, True)),\n",
       " ((7, 0, 5, 3), (False, True, True, False)),\n",
       " ((5, 3, 7, 0), (False, True, False, True)),\n",
       " ((5, 7, 0, 3), (True, True, True, True)),\n",
       " ((0, 5, 7, 3), (True, False, False, False)),\n",
       " ((7, 3, 5, 0), (True, False, False, True)),\n",
       " ((5, 7, 3, 0), (False, False, True, False)),\n",
       " ((7, 3, 5, 0), (False, True, True, False)),\n",
       " ((3, 5, 7, 0), (True, True, True, True)),\n",
       " ((7, 5, 0, 3), (False, True, True, True)),\n",
       " ((0, 3, 7, 5), (False, True, False, True)),\n",
       " ((0, 3, 5, 7), (False, True, True, False)),\n",
       " ((3, 7, 0, 5), (False, True, True, False)),\n",
       " ((7, 3, 0, 5), (False, False, False, False)),\n",
       " ((0, 5, 3, 7), (False, True, True, True)),\n",
       " ((0, 3, 5, 7), (True, False, False, True)),\n",
       " ((5, 3, 0, 7), (True, True, True, False)),\n",
       " ((0, 7, 3, 5), (True, True, True, True)),\n",
       " ((3, 0, 7, 5), (False, True, False, True)),\n",
       " ((7, 0, 3, 5), (True, True, False, True)),\n",
       " ((7, 5, 0, 3), (True, False, False, False)),\n",
       " ((0, 3, 7, 5), (True, False, True, False)),\n",
       " ((5, 0, 7, 3), (False, True, False, False)),\n",
       " ((3, 7, 5, 0), (False, False, True, True)),\n",
       " ((7, 5, 3, 0), (True, False, True, True)),\n",
       " ((5, 0, 3, 7), (True, True, True, False)),\n",
       " ((3, 0, 5, 7), (True, True, True, True)),\n",
       " ((3, 0, 5, 7), (False, False, False, False)),\n",
       " ((3, 5, 0, 7), (False, False, False, True)),\n",
       " ((7, 3, 0, 5), (True, True, True, True)),\n",
       " ((0, 7, 5, 3), (True, True, False, False)),\n",
       " ((5, 7, 0, 3), (False, False, False, False)),\n",
       " ((7, 0, 5, 3), (True, False, False, True)),\n",
       " ((0, 7, 3, 5), (False, False, False, False)),\n",
       " ((0, 5, 7, 3), (False, True, True, True))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459b39df-8f98-4374-a063-a96379fe2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "################ VALIDATION #################\n",
    "#############################################\n",
    "\n",
    "# Establecer la semilla\n",
    "seed(68)\n",
    "\n",
    "# Paso 1: Generar permutaciones de los datasets\n",
    "nums_validation = [1, 6, 9]\n",
    "perm_validation = list(permutations(nums_validation))\n",
    "shuffle(perm_validation)\n",
    "\n",
    "# Paso 2: Generar permutaciones del tipo de dataset\n",
    "\n",
    "# Generar una lista de booleanos con 50% True y 50% False (MISMO TAMAÑO QUE perm_validation)\n",
    "total_booleans = len(perm_validation) * 3  \n",
    "num_true = total_booleans // 2         \n",
    "num_false = total_booleans - num_true  \n",
    "# Crear lista\n",
    "bool_flat_list = [True] * num_true + [False] * num_false\n",
    "# Mezclar aleatoriamente\n",
    "shuffle(bool_flat_list)\n",
    "# Dividir en tuplas de 3 elementos\n",
    "tipoDataset_validation = [tuple(bool_flat_list[i:i+3]) for i in range(0, total_booleans, 3)]\n",
    "\n",
    "# Paso 3: Duplicación de permutaciones invirtiendo el tipo (Si antes valía False, ahora vale True y viceversa)\n",
    "\n",
    "# Lista complementaria de tipoDataset_validation\n",
    "tipoDataset_validation_inv = [tuple(not x for x in comb) for comb in tipoDataset_validation]\n",
    "perm_validation_extended = perm_validation * 2                                    # Duplicamos las permutaciones\n",
    "tipoDataset_validation_extended = tipoDataset_validation + tipoDataset_validation_inv  # Unimos las listas complementarias\n",
    "\n",
    "# Paso 4: Emparejar para no perder la correspondencia, barajamos y extraemos las listas\n",
    "dataset_validation = list(zip(perm_validation_extended, tipoDataset_validation_extended))\n",
    "shuffle(dataset_validation)\n",
    "perm_validation_extended, tipoDataset_validation_extended = zip(*dataset_validation)\n",
    "perm_validation_extended = list(perm_validation_extended)\n",
    "tipoDataset_validation_extended = list(tipoDataset_validation_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a4fe2c-67e0-43c1-9269-8176f8666922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((6, 9, 1), (False, False, False)),\n",
       " ((9, 1, 6), (False, False, True)),\n",
       " ((6, 9, 1), (True, True, True)),\n",
       " ((1, 6, 9), (False, True, True)),\n",
       " ((1, 6, 9), (True, False, False)),\n",
       " ((1, 9, 6), (False, False, True)),\n",
       " ((6, 1, 9), (False, False, True)),\n",
       " ((6, 1, 9), (True, True, False)),\n",
       " ((9, 6, 1), (False, True, True)),\n",
       " ((1, 9, 6), (True, True, False)),\n",
       " ((9, 6, 1), (True, False, False)),\n",
       " ((9, 1, 6), (True, True, False))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0728733-f43f-4176-8732-7faecd605f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "################### TEST ####################\n",
    "#############################################\n",
    "\n",
    "# Establecer la semilla\n",
    "seed(42)\n",
    "\n",
    "# Paso 1: Generar permutaciones de los datasets\n",
    "nums_test = [2, 4, 8]\n",
    "perm_test = list(permutations(nums_test))\n",
    "shuffle(perm_test)\n",
    "\n",
    "# Paso 2: Generar permutaciones del tipo de dataset\n",
    "\n",
    "# Generar una lista de booleanos con 50% True y 50% False (MISMO TAMAÑO QUE perm_test)\n",
    "total_booleans = len(perm_test) * 3  \n",
    "num_true = total_booleans // 2         \n",
    "num_false = total_booleans - num_true  \n",
    "# Crear lista\n",
    "bool_flat_list = [True] * num_true + [False] * num_false\n",
    "# Mezclar aleatoriamente\n",
    "shuffle(bool_flat_list)\n",
    "# Dividir en tuplas de 3 elementos\n",
    "tipoDataset_test = [tuple(bool_flat_list[i:i+3]) for i in range(0, total_booleans, 3)]\n",
    "\n",
    "# Paso 3: Duplicación de permutaciones invirtiendo el tipo (Si antes valía False, ahora vale True y viceversa)\n",
    "\n",
    "# Lista complementaria de tipoDataset_test\n",
    "tipoDataset_test_inv = [tuple(not x for x in comb) for comb in tipoDataset_test]\n",
    "perm_test_extended = perm_test * 2                                    # Duplicamos las permutaciones\n",
    "tipoDataset_test_extended = tipoDataset_test + tipoDataset_test_inv  # Unimos las listas complementarias\n",
    "\n",
    "# Paso 4: Emparejar para no perder la correspondencia, barajamos y extraemos las listas\n",
    "dataset_test = list(zip(perm_test_extended, tipoDataset_test_extended))\n",
    "shuffle(dataset_test)\n",
    "perm_test_extended, tipoDataset_test_extended = zip(*dataset_test)\n",
    "perm_test_extended = list(perm_test_extended)\n",
    "tipoDataset_test_extended = list(tipoDataset_test_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0a6b21-8937-4dd5-ae20-65bc493bebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((8, 4, 2), (True, False, True)),\n",
       " ((2, 8, 4), (False, False, False)),\n",
       " ((8, 2, 4), (True, False, True)),\n",
       " ((4, 8, 2), (True, True, True)),\n",
       " ((4, 2, 8), (False, True, True)),\n",
       " ((2, 4, 8), (False, True, False)),\n",
       " ((2, 8, 4), (True, True, True)),\n",
       " ((2, 4, 8), (True, False, True)),\n",
       " ((4, 8, 2), (False, False, False)),\n",
       " ((8, 4, 2), (False, True, False)),\n",
       " ((8, 2, 4), (False, True, False)),\n",
       " ((4, 2, 8), (True, False, False))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255b2611-cc75-4795-afbb-53abfd55116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d79a7f-aa98-40cb-aaf1-34313e0f6dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e3f78a-81f4-43ef-8828-4fc4929c8a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3ebe3-cce3-4622-a415-e99340767da3",
   "metadata": {},
   "source": [
    "### FUNCIONES PARA LA CREACIÓN DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921911bf-c268-4de7-b1fd-01277dfce4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dataset_ID_column(df, i):\n",
    "    df['dataset_ID'] = i\n",
    "    return df\n",
    "\n",
    "def add_interval_column(df, long_interval):\n",
    "    global EXECUTION\n",
    "    bins = range(0, 3000 + long_interval, long_interval)  # 0 a 3000 ms en intervalos de long_interval ms\n",
    "    # Agregamos una nueva columna al DataFrame con las etiquetas de los intervalos\n",
    "    l = 3000 // long_interval\n",
    "    df.loc[:, 'interval'] = EXECUTION * l + (pd.cut(df['timestamps'], bins=bins, labels=False) + 1)\n",
    "    return df\n",
    "\n",
    "def delete_unnecessary_columns(df):\n",
    "    df_nuevo = df[[\"timestamps\", \"attack\"]].copy()\n",
    "    return df_nuevo\n",
    "\n",
    "def add_spikes_column(df):\n",
    "    df = df.groupby(['dataset_ID', 'interval', 'attack']).size().reset_index(name='spikes')\n",
    "    return df\n",
    "\n",
    "def add_class(df, tipo, long_interval):\n",
    "    global EXECUTION\n",
    "    df['class'] = 0\n",
    "    if tipo:\n",
    "        l = 3000 // long_interval\n",
    "        intervalo = EXECUTION * l + math.ceil(625 / long_interval)\n",
    "        df.loc[df['interval'] == intervalo, 'class'] = 1\n",
    "    return df\n",
    "\n",
    "def add_new_predictors(df):\n",
    "    df.loc[:, 'differenceSpikes_1'] = df['spikes'].diff().fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "def lectura_datasets(long_interval, lista, tipo):\n",
    "    global EXECUTION\n",
    "\n",
    "    # Rutas base del archivo CSV para ambos casos\n",
    "    base_path_true = \"./data/csv/output_FLO_quarter_flash_trial_{}_BKG_trial_9{}/spikes.csv\"\n",
    "    base_path_false = \"./data/csv/output_flash_trial_{}_BKG_trial_9{}/spikes.csv\"\n",
    "\n",
    "    # DataFrame vacío para almacenar todos los datos\n",
    "    combined_dataset = pd.DataFrame()\n",
    "\n",
    "    for i, t in zip(lista, tipo):\n",
    "        # Seleccionar la ruta base según el valor de la lista tipo\n",
    "        base_path = base_path_true if t else base_path_false\n",
    "\n",
    "        # Genera la ruta completa del archivo CSV para el valor actual de i\n",
    "        file_path = base_path.format(i, i)\n",
    "\n",
    "        try:\n",
    "            # Lee el archivo CSV\n",
    "            df = pd.read_csv(file_path, delimiter=\";\")\n",
    "            df = delete_unnecessary_columns(df)\n",
    "            df = add_dataset_ID_column(df, i)\n",
    "            df = add_interval_column(df, long_interval)\n",
    "            df = add_spikes_column(df)\n",
    "            df = add_class(df, t, long_interval)\n",
    "            # Concatena el dataframe al conjunto de datos consolidado\n",
    "            combined_dataset = pd.concat([combined_dataset, df], ignore_index=True)\n",
    "            EXECUTION = EXECUTION + 1\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archivo no encontrado para i = {i}\")\n",
    "\n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31279845-e76a-460d-b679-3c2ad6ad5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para el conteo de los intervalos y su longitud\n",
    "EXECUTION = 0\n",
    "INTERVAL_LENGTH = 100\n",
    "\n",
    "# Creación del conjunto de train\n",
    "train_df = pd.DataFrame()\n",
    "for perm, tipo in zip(perm_train_extended, tipoDataset_train_extended):\n",
    "    train_df = pd.concat([train_df, lectura_datasets(INTERVAL_LENGTH, perm, tipo)], ignore_index=True)\n",
    "train_df = add_new_predictors(train_df)\n",
    "train_df.to_csv(\"./data/train.csv\", index = False)\n",
    "\n",
    "# Creación del conjunto de validation\n",
    "validation_df = pd.DataFrame()\n",
    "for perm, tipo in zip(perm_validation_extended, tipoDataset_validation_extended):\n",
    "    validation_df = pd.concat([validation_df, lectura_datasets(INTERVAL_LENGTH, perm, tipo)], ignore_index=True)\n",
    "validation_df = add_new_predictors(validation_df)\n",
    "validation_df.to_csv(\"./data/validation.csv\", index = False)\n",
    "\n",
    "# Creación del conjunto de test\n",
    "test_df = pd.DataFrame()\n",
    "for perm, tipo in zip(perm_test_extended, tipoDataset_test_extended):\n",
    "    test_df = pd.concat([test_df, lectura_datasets(INTERVAL_LENGTH, perm, tipo)], ignore_index=True)\n",
    "test_df = add_new_predictors(test_df)\n",
    "test_df.to_csv(\"./data/test.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
